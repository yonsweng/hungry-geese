{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('hungry-geese': conda)",
   "metadata": {
    "interpreter": {
     "hash": "05713f8b191da0d44636ec07dbc4decc6776412276f73b82323e3790c6ed24fb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission.py\n",
    "import base64\n",
    "import pickle\n",
    "import zlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.container import Sequential\n",
    "from torch.distributions.categorical import Categorical\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Action\n",
    "\n",
    "\n",
    "class FeaturesExtractor(nn.Module):    \n",
    "    def __init__(self, features_dim=64*7*11):\n",
    "        super().__init__()\n",
    "        n_channels = 11\n",
    "        self.conv1 = nn.Conv2d(n_channels, 32, (3, 3))\n",
    "        self.conv2 = nn.Conv2d(32, 64, (3, 3))\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MlpExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MlpExtractor, self).__init__()\n",
    "        self.shared_net = Sequential(\n",
    "            nn.Linear(64*7*11, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_net = Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.value_net = Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared_net(x)\n",
    "        p = self.policy_net(x)\n",
    "        v = self.value_net(x)\n",
    "        return p, v\n",
    "\n",
    "\n",
    "class ActorCriticPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCriticPolicy, self).__init__()\n",
    "        self.features_extractor = FeaturesExtractor()\n",
    "        self.mlp_extractor = MlpExtractor()\n",
    "        self.action_net = nn.Linear(in_features=64, out_features=4, bias=True)\n",
    "        self.value_net = nn.Linear(in_features=64, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        p, v = self.mlp_extractor(x)\n",
    "        p = self.action_net(p)\n",
    "        v = self.value_net(v)\n",
    "        return p, v\n",
    "\n",
    "\n",
    "state_dict = _STATE_DICT_\n",
    "\n",
    "state_dict = pickle.loads(zlib.decompress(base64.b64decode(state_dict)))\n",
    "model = ActorCriticPolicy()\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "obs_prev = None\n",
    "act_prev = None\n",
    "\n",
    "\n",
    "# Modified from https://www.kaggle.com/yuricat/smart-geese-trained-by-reinforcement-learning\n",
    "def process_obs(obs):\n",
    "    global obs_prev\n",
    "    obs_index = obs.index\n",
    "\n",
    "    b = np.zeros((7 * 11, 11), dtype=np.uint8)\n",
    "    b[:, -1] = 255  # empty cells\n",
    "\n",
    "    for p, pos_list in enumerate(obs.geese):\n",
    "        # head position\n",
    "        for pos in pos_list[:1]:\n",
    "            b[pos, 0 + (p - obs_index) % 4] = 255\n",
    "            b[pos, -1] = 0\n",
    "        # whole position\n",
    "        for pos in pos_list:\n",
    "            b[pos, 4 + (p - obs_index) % 4] = 255\n",
    "            b[pos, -1] = 0\n",
    "\n",
    "    # previous head position\n",
    "    if obs_prev is not None:\n",
    "        for pos in obs_prev.geese[obs_index][:1]:\n",
    "            b[pos, -3] = 255\n",
    "\n",
    "    for pos in obs.food:\n",
    "        b[pos, -2] = 255\n",
    "\n",
    "    b = b.reshape(7, 11, -1)\n",
    "\n",
    "    b = np.concatenate([b[:, -2:],\n",
    "                        b,\n",
    "                        b[:, :2]], axis=1)\n",
    "    b = np.concatenate([b[-2:, :],\n",
    "                        b,\n",
    "                        b[:2, :]], axis=0)\n",
    "    return b\n",
    "\n",
    "\n",
    "def agent(obs, conf):\n",
    "    global model, obs_prev, act_prev\n",
    "    obs_backup = obs\n",
    "    obs = process_obs(obs) / 255.  # normalize\n",
    "    obs = np.expand_dims(obs.transpose(2, 0, 1), 0)\n",
    "    obs = torch.from_numpy(obs.astype(np.float32))\n",
    "    p, v = model(obs)\n",
    "    p = p.squeeze()\n",
    "    print(p)\n",
    "    p = F.softmax(p, dim=0)\n",
    "    if act_prev is not None:\n",
    "        act_oppo = (act_prev + 1) % 4 + 1\n",
    "        p[act_oppo - 1] = 0\n",
    "        p /= p.sum()\n",
    "    # action = p.squeeze().argmax().item() + 1\n",
    "    action = Categorical(p).sample().item() + 1\n",
    "    obs_prev = obs_backup\n",
    "    act_prev = action\n",
    "    return Action(action).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import base64\n",
    "import pickle\n",
    "import zlib\n",
    "from stable_baselines3 import PPO\n",
    "from kaggle_environments import make\n",
    "\n",
    "path = 'models7'\n",
    "list_of_files = glob.glob(os.path.join(path, '*.zip'))\n",
    "model_path = max(list_of_files, key=os.path.getmtime)\n",
    "print(model_path)\n",
    "model = PPO.load(model_path)\n",
    "# print(model.policy)\n",
    "\n",
    "state_dict = model.policy.to('cpu').state_dict()\n",
    "state_dict = base64.b64encode(zlib.compress(pickle.dumps(state_dict)))\n",
    "\n",
    "with open('submission.py', 'r') as file:\n",
    "    src = file.read()\n",
    "src = src.replace(\"_STATE_DICT_\", f\"{state_dict}\")\n",
    "with open('submission.py', 'w') as file:\n",
    "    file.write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = make(\"hungry_geese\", debug=True)\n",
    "# env.run([\"examples/risk_averse.py\", \"examples/mighty_boiler_goose.py\", \"examples/simple_bfs.py\", \"submission.py\"])\n",
    "env.run([\"submission.py\", \"submission.py\", \"submission.py\", \"submission.py\"])\n",
    "env.render(mode=\"ipython\", width=700, height=550)\n",
    "# white, blue, green, red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c hungry-geese -f submission.py -m \"PPO MlpPolicy(ReLU) self-play\""
   ]
  }
 ]
}